{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import re\n",
    "\n",
    "import plotly.express as px\n",
    "%pylab inline\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_columns = ['SH_CDATE', 'GAS_ACQ_DATE', 'D_G_ACQ_DATE']\n",
    "year_columns = ['WH_SPUD_YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil = pd.read_csv('technathon2019/challenge2/Geochemistry Data/CNS oil.csv', skiprows=[1])\n",
    "rocks = pd.read_csv('technathon2019/challenge2/Geochemistry Data/CNS rock samples.csv', skiprows=[1], low_memory=False)\n",
    "gas_train = pd.read_csv('technathon2019/challenge2/Geochemistry Data/CNS_gas_train.csv', skiprows=[1],\n",
    "                        parse_dates=time_columns)\n",
    "gas_test = pd.read_csv('technathon2019/challenge2/Geochemistry Data/CNS_gas_test.csv', skiprows=[1])\n",
    "\n",
    "gas_train['WH_SPUD_YEAR'] = gas_train['WH_SPUD_YEAR'].fillna(1900).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_train[['WH_SPUD_YEAR']].profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil.iloc(1)[:2].profile_report(style={'full_width':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = gas_train.profile_report(style={'full_width':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_file('train_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = (rocks, oil, gas_train, gas_test)\n",
    "dataset_names = ['rocks', 'oil', 'gas_train', 'gas_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = {}\n",
    "cat_features = {}\n",
    "id_columns = {}\n",
    "date_columns = {}\n",
    "for ds_name, ds in zip(dataset_names, datasets):\n",
    "    text_features[ds_name] = [f for f, dt in ds.dtypes.items() if dt == np.dtype('O') and f not in time_columns]\n",
    "    cat_features[ds_name] = [f for f, fc in ds.nunique().items() if fc < 100 and f not in text_features[ds_name]]\n",
    "    id_columns[ds_name] = [c for c in ds.columns if '_ID_' in c]\n",
    "    date_columns[ds_name] = [c for c in ds.columns if 'DATE' in c or c in time_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_name, ds in zip(dataset_names, datasets):\n",
    "    for c in ds.columns:\n",
    "        if c in text_features[ds_name]:\n",
    "            ds.loc[:, c] = ds[c].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = 'GAS_C1, GAS_C2, GAS_C3, GAS_IC4, GAS_NC4, GAS_IC5, GAS_NC5'.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = {}\n",
    "for ds_name, ds in zip(dataset_names, datasets):\n",
    "    feature_columns[ds_name] = list(np.setdiff1d(ds.columns, \n",
    "                       np.concatenate([target_columns, time_columns, text_features[ds_name], \n",
    "                                       date_columns[ds_name], year_columns, id_columns[ds_name]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = gas_train[feature_columns]\n",
    "X_test = gas_test[feature_columns]\n",
    "\n",
    "y_train = gas_train[target_columns]\n",
    "y_test = gas_test[target_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "# from catboost.eval.catboost_evaluation import *\n",
    "\n",
    "train_dataset = Pool(data=X_train[[c for c in X_train.columns if c not in time_columns + text_features]], label=y_train   \n",
    "#                , cat_features=text_features\n",
    "              )\n",
    "test_dataset = Pool(data=X_test[[c for c in X_test.columns if c not in time_columns + text_features]]\n",
    "#                , text_features=text_features\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_model = CatBoostRegressor(iterations=1000, depth=2, objective='MultiRMSE')\n",
    "cb_model.fit(train_dataset, \n",
    "             verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.Series(cb_model.get_feature_importance(), \n",
    "                               index=pd.Index(train_dataset.get_feature_names(), name='feature'), name='feature_importance').sort_values()[::-1]\n",
    "\n",
    "fig = px.bar(feature_importance.reset_index(), x='feature', y='feature_importance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product, chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cross_val(X, y, X_test, param, cat_features, n_splits=3):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    acc = []\n",
    "    predict = None\n",
    "    \n",
    "    for tr_ind, val_ind in skf.split(X, y):\n",
    "        X_train = X[tr_ind]\n",
    "        y_train = y[tr_ind]\n",
    "        \n",
    "        X_valid = X[val_ind]\n",
    "        y_valid = y[val_ind]\n",
    "        \n",
    "        clf = CatBoostClassifier(iterations=500,\n",
    "                                loss_function = param['loss_function'],\n",
    "                                depth=param['depth'],\n",
    "                                l2_leaf_reg = param['l2_leaf_reg'],\n",
    "#                                 eval_metric = 'Accuracy',\n",
    "                                leaf_estimation_iterations = 10,\n",
    "                                use_best_model=True,\n",
    "                                logging_level='Silent'\n",
    "        )\n",
    "        \n",
    "        clf.fit(X_train, \n",
    "                y_train,\n",
    "                cat_features=cat_features,\n",
    "                eval_set=(X_valid, y_valid)\n",
    "        )\n",
    "        \n",
    "        y_pred = clf.predict(X_valid)\n",
    "        accuracy = accuracy_score(y_valid, y_pred)\n",
    "        acc.append(accuracy)\n",
    "    return sum(acc)/n_splits\n",
    "    \n",
    "def catboost_GridSearchCV(X, y, X_test, params, cat_features, n_splits=5):\n",
    "    ps = {'acc':0,\n",
    "          'param': []\n",
    "    }\n",
    "    \n",
    "    predict=None\n",
    "    \n",
    "    for prms in tqdm(list(ParameterGrid(params)), ascii=True, desc='Params Tuning:'):\n",
    "                          \n",
    "        acc = cross_val(X, y, X_test, prms, cat_features, n_splits=5)\n",
    "\n",
    "        if acc>ps['acc']:\n",
    "            ps['acc'] = acc\n",
    "            ps['param'] = prms\n",
    "    print('Acc: '+str(ps['acc']))\n",
    "    print('Params: '+str(ps['param']))\n",
    "    \n",
    "    return ps['param']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production = pd.read_csv('technathon2019/challenge2/Production Data/CNS_Field_Production.csv',\n",
    "                   parse_dates=['PERIODDATE'], dtype={'PERIODYR': int})\n",
    "production['FIELDNAME'] = production['FIELDNAME'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_geo = production.groupby('FIELDNAME')[['X', 'Y']].first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_vol = production.groupby(['FIELDNAME', 'PERIODYR'])['WATPRODVOL'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = prod_vol.to_frame().join(fields_geo).reset_index().fillna(0.0).sort_values(['PERIODYR', 'FIELDNAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.set_mapbox_access_token(open('.mapbox_token').read())\n",
    "fig = px.scatter_mapbox(plot_data, lat='Y', lon='X', size='WATPRODVOL',\n",
    "                        animation_frame='PERIODYR', animation_group='FIELDNAME', zoom=4,\n",
    "                        width=512, height=512\n",
    "                        )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('map.html', 'w') as f:\n",
    "    f.write(fig.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production geochemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in datasets:\n",
    "    ds['FIELD_NAME'] = ds['WH_FIELD'].astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "field_name_mapping = defaultdict(lambda: set())\n",
    "field_name_mapping_inv = defaultdict(lambda: set())\n",
    "\n",
    "for fn in np.concatenate([oil['FIELD_NAME'].unique(), rocks['FIELD_NAME'].unique(), \n",
    "                          gas_train['FIELD_NAME'].unique(), gas_test['FIELD_NAME'].unique()]):\n",
    "    m = re.match('(.+)\\s+\\((.+)\\)', fn)\n",
    "    \n",
    "    if m:\n",
    "        well_name, field_name = m.groups()\n",
    "        if field_id not in field_name_mapping:\n",
    "            field_name_mapping[well_name].add(field_name)\n",
    "            field_name_mapping_inv[field_name].add(well_name)\n",
    "\n",
    "field_name_mapping = {k: v.pop() for k, v in field_name_mapping.items()}\n",
    "field_name_mapping_inv = {k: v.pop() for k, v in field_name_mapping_inv.items()}\n",
    "\n",
    "def extract_field_name(s):\n",
    "    m = re.match('.+\\s+\\((.+)\\)', s)\n",
    "    if m:\n",
    "        return m.groups()[0]\n",
    "    elif s in field_name_mapping:\n",
    "        return field_name_mapping[s]\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def extract_well_name(s):\n",
    "    m = re.match('(.+)\\s+\\(.+\\)', s)\n",
    "    if m:\n",
    "        return m.groups()[0]\n",
    "    elif s in field_name_mapping_inv:\n",
    "        return field_name_mapping_inv[s]\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in datasets:\n",
    "    ds['FIELDNAME'] = ds['FIELD_NAME'].apply(extract_field_name)\n",
    "    ds['WELLNAME'] = ds['FIELD_NAME'].apply(extract_well_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod[prod.FIELDNAME == 'cook']['PERIODYR'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_train[gas_train.FIELDNAME == 'cook']['WH_SPUD_YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil[oil['FIELDNAME'] == 'curlew c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_field_years = gas_train[['FIELDNAME', 'WH_SPUD_YEAR']][\n",
    "    gas_train['FIELDNAME'].isin(prod.FIELDNAME.unique()) & ~gas_train[['FIELDNAME', 'WH_SPUD_YEAR']].duplicated()] \\\n",
    "    .sort_values('FIELDNAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_field_years = oil[['FIELDNAME', 'WH_SPUD_YEAR']][\n",
    "    oil['FIELDNAME'].isin(prod.FIELDNAME.unique()) & ~oil[['FIELDNAME', 'WH_SPUD_YEAR']].duplicated()] \\\n",
    "    .sort_values('FIELDNAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_field_years.sort_values(['FIELDNAME', 'WH_SPUD_YEAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave latest geochemistry year\n",
    "for ds in datasets:\n",
    "    ds = ds.merge(ds.groupby('FIELDNAME')['WH_SPUD_YEAR'].max(skipna=True).rename('MAX_SPUD_YEAR'), on='FIELDNAME')\n",
    "    ds = ds[ds['WH_SPUD_YEAR'] == ds['MAX_SPUD_YEAR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_fields = ['OILPRODMBD', 'AGASPROMMS', 'WATPRODMBD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_fields = {c: 'sum' for c in prod_fields}\n",
    "agg_fields.update({c: 'mean' for c in ['X', 'Y']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_performance = production[['FIELDNAME', 'PERIODYR', 'X', 'Y'] + prod_fields] \\\n",
    "    .groupby(['FIELDNAME', 'PERIODYR'])[['X', 'Y'] + prod_fields].agg(agg_fields) \\\n",
    "    .groupby('FIELDNAME')[['X', 'Y'] + prod_fields].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {}\n",
    "for ds_name, ds in zip(dataset_names, datasets):\n",
    "    performance[ds_name] = prod_performance.merge(ds, how='right', on='FIELDNAME')[\n",
    "        ['FIELDNAME', 'X', 'Y'] + feature_columns[ds_name] + prod_fields].dropna(subset=prod_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gas Performance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_columns = ['WH_TD_M', 'WH_LONG', 'WH_LAT', 'WH_DR_ELEV_M', 'SH_DEPTH_BOT_FT', 'SH_DEPTH_TOP_FT', 'SH_FORM_BOT',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns['oil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_model.get_feature_importance(train_dataset, fstr_type='ShapValues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "# from catboost.eval.catboost_evaluation import *\n",
    "ds_name = 'oil'\n",
    "train_dataset = Pool(data=performance[ds_name][[c for c in feature_columns[ds_name] if c not in filter_columns]], label=performance[ds_name][prod_fields]  \n",
    "#                , cat_features=text_features[ds_name]\n",
    "              )\n",
    "\n",
    "cb_model = CatBoostRegressor(iterations=2000, depth=2, objective='MultiRMSE')\n",
    "cb_model.fit(train_dataset, verbose=100)\n",
    "\n",
    "feature_importance = pd.Series(cb_model.get_feature_importance(), \n",
    "                               index=pd.Index(train_dataset.get_feature_names(), name='feature'), name='feature_importance').sort_values()[::-1]\n",
    "\n",
    "fig = px.bar(feature_importance.reset_index().iloc[:20], x='feature', y='feature_importance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.index[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_features = feature_importance.index[:5].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_anomalies = oil[['FIELDNAME', 'WH_LAT', 'WH_LONG', 'SH_DEPTH_TOP_FT', 'SH_DEPTH_BOT_FT'] + anomaly_features] \\\n",
    "    .dropna(subset=['WH_LAT', 'WH_LONG', 'SH_DEPTH_TOP_FT', 'SH_DEPTH_BOT_FT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_anomalies = oil_anomalies[oil_anomalies['FIELDNAME'] != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_anomalies['DEPTH'] = (oil_anomalies['SH_DEPTH_BOT_FT'] + oil_anomalies['SH_DEPTH_TOP_FT']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_anomalies.to_csv('oil_anomalies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_anomalies['FIELDNAME'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = oil_anomalies[~oil_anomalies['OIL_PRPH'].isna()][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
